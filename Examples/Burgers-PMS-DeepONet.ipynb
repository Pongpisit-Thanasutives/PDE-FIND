{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e1f133-f1d3-42c6-9c77-3615cb10d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys; sys.path.append('../')\n",
    "from misc import h5file\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import scipy.io as sio\n",
    "\n",
    "import torch, sympytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from siren_pytorch import SirenNet\n",
    "\n",
    "import pysindy as ps\n",
    "\n",
    "from sympy import symbols, simplify, lambdify\n",
    "from mathparser import math_eval\n",
    "from varname import nameof\n",
    "\n",
    "import sys; sys.path.append('../optimizers/')\n",
    "from Adan import Adan\n",
    "from FNO_Adam import Adam\n",
    "\n",
    "MAIN_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a977d096-e837-4b20-98a7-32232f0866b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_pre', 'best_subsets', 'y_pre']\n"
     ]
    }
   ],
   "source": [
    "X_pre, best_subsets, y_pre = h5file(file_path=\"../PMS_data/burgers_pms.h5\", \n",
    "                                    mode='r', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c462555-3669-4aeb-9fb1-8d6c7e499677",
   "metadata": {},
   "outputs": [],
   "source": [
    "un = sio.loadmat(\"../RDAE_data/l21rdae_noisy40_burgers_0.5.mat\")[\"rdae_denoised_un\"].real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcefd6e7-079d-458e-b952-891a52373d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('../Datasets/burgers.mat')\n",
    "x = (data['x'][0]).real\n",
    "t = (data['t'][:,0]).real\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.asarray([X, T]).T\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8deb99a-b9b4-47a0-afa1-58d7618f8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"../PMS_data/burgers_pms_feature_names.yaml\", 'r') as f:\n",
    "    config = yaml.load(f, yaml.Loader)\n",
    "f.close()\n",
    "encoded_feature_names = config[\"encoded_feature_names\"]\n",
    "encoded_pde_names = config[\"encoded_pde_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b1f166-6fbc-431f-b446-d507cfc5e599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u*u_1',\n",
       " 'u_11+u*u_1',\n",
       " 'u_11+u*u_1+u*u_11',\n",
       " 'u_11+u*u_1+u*u_11+u*u*u_11',\n",
       " 'u+u_11+u*u_1+u*u_11+u*u*u_11',\n",
       " 'u+u_11+u*u_1+u*u*u_1+u*u_11+u*u*u_11',\n",
       " 'u+u*u+u_11+u*u_1+u*u*u_1+u*u_11+u*u*u_11',\n",
       " 'u+u*u+u_1+u_11+u*u_1+u*u*u_1+u*u_11+u*u*u_11']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_pde_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e290a14e-bff1-4d98-8824-7394dc44cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like_value(prediction, ground):                                                                                                               \n",
    "    nobs = float(ground.shape[0])\n",
    "    nobs2 = nobs / 2.0\n",
    "    ssr = np.sum(np.abs(ground - prediction)**2)\n",
    "    llf = -nobs2 * np.log(2 * np.pi) - nobs2 * np.log(ssr / nobs) - nobs2\n",
    "    return llf\n",
    "\n",
    "def BIC_AIC(prediction, ground, nparams, reg_func = lambda x: x):\n",
    "    nparams = reg_func(nparams)\n",
    "    llf = log_like_value(prediction, ground)\n",
    "    return -2*llf + np.log(ground.shape[0])*nparams, -2*llf + 2*nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3193f8ba-74df-4b6a-ac18-bcdbb3a0935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalConstraintCalculator(nn.Module):\n",
    "    def __init__(self, symbolic_module, basic_vars, init_coefficients=None, learnable_coefficients=False):\n",
    "        super(PhysicalConstraintCalculator, self).__init__()\n",
    "        self.symbolic_module = symbolic_module\n",
    "        self.basic_vars = basic_vars\n",
    "        \n",
    "        self.coefficients = init_coefficients\n",
    "        self.learnable_coefficients = learnable_coefficients\n",
    "\n",
    "        if self.coefficients is None:\n",
    "            self.coefficients = torch.ones(len(symbolic_module.sympy())).float()\n",
    "        else:\n",
    "            self.coefficients = torch.tensor(data=self.coefficients).float()\n",
    "        self.coefficients = nn.Parameter(self.coefficients).requires_grad_(self.learnable_coefficients)\n",
    "        \n",
    "        # printing\n",
    "        if self.learnable_coefficients: print(\"Learnable coefficients:\", self.coefficients)\n",
    "        else: print(\"NOT learnable coefficients:\", self.coefficients)\n",
    "        print(symbolic_module.sympy())\n",
    "        print(\"Basic variables:\", self.basic_vars)\n",
    "\n",
    "    def set_learnable_coefficients(self, learn):\n",
    "        self.coefficients.requires_grad_(learn)\n",
    "    \n",
    "    def forward(self, input_dict):\n",
    "        return self.symbolic_module(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27cf3237-bfb5-4215-8275-a22845c6b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Sine, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, dimensions, bias=True, activation_function=nn.Tanh(), bn=None, dropout=None):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        # setup ModuleList\n",
    "        self.model  = nn.ModuleList()\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.model.append(nn.Linear(dimensions[i], dimensions[i+1], bias=bias))\n",
    "            if bn is not None and i!=len(dimensions)-2:\n",
    "                self.model.append(bn(dimensions[i+1]))\n",
    "                if dropout is not None:\n",
    "                    self.model.append(dropout)\n",
    "            if i==len(dimensions)-2: break\n",
    "            self.model.append(activation_function)\n",
    "        # weight init\n",
    "        self.model.apply(self.xavier_init)\n",
    "\n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.model): \n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee2c43d-82d1-42d3-8696-0ffd0d23a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet(nn.Module):\n",
    "    def __init__(self, solver, sensor_network, physics_calculator, lb, ub, \n",
    "                 domain_dimension=None, weak_pde_lib=None, effective_indices=None):\n",
    "        super(DeepONet, self).__init__()\n",
    "        self.solver = solver\n",
    "        self.solver_last_layer = nn.Linear(50,1,bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.solver_last_layer.weight)\n",
    "        self.sensor_network = sensor_network\n",
    "        self.m0 = nn.Parameter(torch.tensor([1.0]).float())\n",
    "        self.physics_calculator = physics_calculator\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        # Only to use weak_loss\n",
    "        # spatial x temporal\n",
    "        self.domain_dimension = domain_dimension\n",
    "        self.weak_pde_lib = weak_pde_lib\n",
    "        self.effective_indices = effective_indices\n",
    "        self.weak_coeff_buffer = None\n",
    "        \n",
    "    def forward(self, x, t, u0):\n",
    "        loc = self.solver(self.input_normalize(torch.cat([x, t],  dim=-1)))\n",
    "        G = self.sensor_network(u0)\n",
    "        # torch.einsum(\"bi,ni->nb\", G, loc)+self.b0\n",
    "        return self.solver_last_layer(torch.sin(loc))+(self.m0*torch.matmul(loc, G.T))\n",
    "\n",
    "    def calculate_physics(self, x, t, u0):\n",
    "        u = self.forward(x, t, u0)\n",
    "        u_t = self.gradients(u, t)[0]\n",
    "        u_1 = self.gradients(u, x)[0]\n",
    "        u_11 = self.gradients(u_1, x)[0]\n",
    "        physics = self.physics_calculator({nameof(u):u, \n",
    "                                           nameof(u_1):u_1, \n",
    "                                           nameof(u_11):u_11})\n",
    "        \n",
    "        return u, u_t, physics\n",
    "    \n",
    "    def loss(self, x, t, u0, y_input):\n",
    "        u, u_t, physics = self.calculate_physics(x, t, u0)\n",
    "        coeff = self.physics_calculator.coefficients\n",
    "        physics = (physics*coeff).sum(axis=-1)\n",
    "        mse = F.mse_loss(u, y_input, reduction='mean')\n",
    "        l_eq = F.mse_loss(u_t, physics, reduction='mean')\n",
    "        return torch.add(mse, l_eq)\n",
    "    \n",
    "    def weak_loss(self, x, t, u0, y_input):\n",
    "        u, u_t, physics = self.calculate_physics(x, t, u0)\n",
    "        coeff = torch.tensor(self.weak_coefficients(u)).float()\n",
    "        physics = (physics*coeff).sum(axis=-1)\n",
    "        mse = F.mse_loss(u, y_input, reduction='mean')\n",
    "        l_eq = F.mse_loss(u_t, physics, reduction='mean')\n",
    "        return torch.add(mse, l_eq)\n",
    "    \n",
    "    def weak_form(self, u):\n",
    "        pred = u.reshape(self.domain_dimension[1], \n",
    "                         self.domain_dimension[0]).T.detach().numpy()\n",
    "        pred = np.expand_dims(pred,-1)\n",
    "        X_weak = self.weak_pde_lib.fit_transform(pred)\n",
    "        y_weak = self.weak_pde_lib.convert_u_dot_integral(pred)\n",
    "        return X_weak, y_weak\n",
    "    \n",
    "    def weak_coefficients(self, u):\n",
    "        np.random.seed(0)\n",
    "        X_weak, y_weak = self.weak_form(u)\n",
    "        X_weak = X_weak[:, self.effective_indices]\n",
    "        self.weak_coeff_buffer = np.linalg.lstsq(X_weak, y_weak, rcond=None)[0].flatten()\n",
    "        return self.weak_coeff_buffer\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, \n",
    "                    grad_outputs=torch.ones(func.shape))\n",
    "\n",
    "    def input_normalize(self, inp):\n",
    "        return -1.0+2.0*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6cfb5d3-a870-4449-bf8c-b760a1d20373",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(seed=0)\n",
    "# sampled_indices_x = rng.choice(len(x), size=int(len(x)//(2)), replace=False)\n",
    "# sampled_indices_t = rng.choice(len(t), size=int(len(t)//(2)), replace=False)\n",
    "sampled_indices_x = np.array([i for i in range(len(x)) if i%2==0])\n",
    "sampled_indices_t = np.array([i for i in range(len(t)) if i%2==0]) \n",
    "domain_dimension = len(sampled_indices_x), len(sampled_indices_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be26291-1b4d-4cf9-9d41-dfce3c1913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(MAIN_SEED);\n",
    "torch.manual_seed(MAIN_SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c632efe7-a59b-4ef1-9939-fb1037932e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = X[sampled_indices_t, :][:, sampled_indices_x]\n",
    "TT = T[sampled_indices_t, :][:, sampled_indices_x]\n",
    "XXTT = XT[sampled_indices_x, :, :][:, sampled_indices_t, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8173a691-9bfb-406b-ab0a-4a3ab27852c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3000; diff_order = 2\n",
    "weak_pde_lib = ps.WeakPDELibrary(library_functions=[lambda x:x, lambda x: x*x], \n",
    "                                 function_names=[lambda x:x, lambda x: x+x], \n",
    "                                 derivative_order=diff_order, p=diff_order, \n",
    "                                 spatiotemporal_grid=XXTT, \n",
    "                                 include_bias=False, is_uniform=True, K=K # new random K points in every calls to the ps.WeakPDELibrary\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99bd1108-8cab-4908-9f84-23226465c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((XX.flatten()[:,None], TT.flatten()[:,None]))\n",
    "y_train = un.T[sampled_indices_t, :][:, sampled_indices_x].flatten()[:,None]\n",
    "lb = torch.tensor(X_train.min(axis=0)).float().requires_grad_(False)\n",
    "ub = torch.tensor(X_train.max(axis=0)).float().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fb011cd-d76f-4614-b134-9a1abeca6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = y_train[(X_train[:, 1:2] == 0.0).flatten(), :]\n",
    "u0 = torch.tensor(u0).float().reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ac6a7c0-6773-4b9a-b6cd-382d767f4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "del XX, TT, XXTT, X, T, XT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbff35fc-258f-4bb9-b71c-543237cbb3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6528, 2]), torch.Size([6528, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to tensors\n",
    "X_train = torch.tensor(X_train).float().requires_grad_(True)\n",
    "y_train = torch.tensor(y_train).float().requires_grad_(False)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f745a25b-5bca-45fd-ac3a-7ee9010277fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.09517879, -1.0106909 ], dtype=float32),\n",
       " SymPyModule(expressions=(u_11, u*u_1)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com = 2; com = max(com, 1)\n",
    "effective_indices = np.where(best_subsets[com-1]>0)[0].tolist()\n",
    "init_coefficients = np.linalg.lstsq(X_pre[:, effective_indices], \n",
    "                                    y_pre, rcond=None)[0].flatten()\n",
    "mod, basic_vars = math_eval(encoded_pde_names[com-1], \n",
    "                            return_torch=True, split_by_addition=True)\n",
    "init_coefficients, mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70d17a5a-5c79-4012-aa86-4a44dee55eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnable coefficients: Parameter containing:\n",
      "tensor([ 0.0952, -1.0107], requires_grad=True)\n",
      "[u_11, u*u_1]\n",
      "Basic variables: ['u', 'u_1', 'u_11']\n"
     ]
    }
   ],
   "source": [
    "# bias init at 0.01 | SIREN\n",
    "\n",
    "solver = TorchMLP([2,50,50,50,50], bn=None, activation_function=Sine())\n",
    "# solver = SirenNet(dim_in=2, dim_hidden=50, dim_out=1, num_layers = 4, \n",
    "#                   w0_initial = 30.)\n",
    "\n",
    "sensor_network = TorchMLP([u0.shape[-1],50,50], bn=None, activation_function=Sine())\n",
    "\n",
    "physics_calculator = PhysicalConstraintCalculator(symbolic_module=mod, \n",
    "                                                  basic_vars=basic_vars, \n",
    "                                                  init_coefficients=init_coefficients, \n",
    "                                                  learnable_coefficients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f2530f7-851b-4c3d-b42b-6b4f61ef432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "don = DeepONet(solver, sensor_network, physics_calculator, \n",
    "                lb, ub, domain_dimension, \n",
    "                weak_pde_lib, effective_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "239324b8-04d0-4c9e-af66-5c77c05876a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.0028746635653078556\n",
      "Epoch 50:  0.0023274405393749475\n",
      "Epoch 99:  0.0023274405393749475\n"
     ]
    }
   ],
   "source": [
    "don.physics_calculator.set_learnable_coefficients(False)\n",
    "lbfgs = torch.optim.LBFGS(don.parameters(), lr=0.1, \n",
    "                          max_iter=500, max_eval=500, history_size=300, \n",
    "                          line_search_fn='strong_wolfe')\n",
    "epochs = 100\n",
    "don.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled(): \n",
    "            lbfgs.zero_grad()\n",
    "        l = don.loss(X_train[:, 0:1], X_train[:, 1:2], u0, y_train)\n",
    "        if l.requires_grad: \n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    lbfgs.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    if (i%50)==0 or i==epochs-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2883872-bd37-4aaf-aed2-920c43b59c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-21082.6181447266, -21096.185856519885)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "don.eval()\n",
    "pred = don(X_train[:, 0:1], X_train[:, 1:2], u0).detach().numpy()\n",
    "BIC_AIC(pred, y_train.detach().numpy(), com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19d7b3f1-9c66-44b7-8ee0-0639e361059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(don.state_dict(), \"../PMS_weights/don_mlp_sine.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076a314-14f1-4df6-b567-299ca6f816dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43e2cc-eafd-432f-a388-9d01e17a21ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a3e27b-7dff-4f8c-aff7-da75eac15782",
   "metadata": {},
   "source": [
    "#### weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40590930-a24c-407a-a0b4-81a00b7fe862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lbfgs...\n",
      "Epoch 0:  0.002327537629753351\n",
      "Epoch 50:  0.002327489433810115\n",
      "Epoch 99:  0.002327489433810115\n",
      "(-21083.277247995444, -21096.84495978873)\n",
      "[ 0.09517879 -1.0106909 ]\n",
      "[ 0.09805153 -1.00114632]\n",
      "using non-lbfgs...\n",
      "Epoch 0:  0.0023282512556761503\n",
      "Epoch 50:  0.002327435417100787\n",
      "Epoch 100:  0.0023273909464478493\n",
      "Epoch 150:  0.002327350666746497\n",
      "Epoch 200:  0.0023273255210369825\n",
      "Epoch 250:  0.002327294321730733\n",
      "Epoch 300:  0.0023272705730050802\n",
      "Epoch 350:  0.0023272496182471514\n",
      "Epoch 400:  0.002327226335182786\n",
      "Epoch 450:  0.0023272098042070866\n",
      "Epoch 500:  0.0023271902464330196\n",
      "Epoch 550:  0.0023271769750863314\n",
      "Epoch 600:  0.0023271599784493446\n",
      "Epoch 650:  0.0023271481040865183\n",
      "Epoch 700:  0.00232713483273983\n",
      "Epoch 750:  0.002327118767425418\n",
      "Epoch 800:  0.0023271040990948677\n",
      "Epoch 850:  0.002327089197933674\n",
      "Epoch 900:  0.002327076392248273\n",
      "Epoch 950:  0.002327065449208021\n",
      "Epoch 1000:  0.002327053342014551\n",
      "Epoch 1050:  0.0023270423989742994\n",
      "Epoch 1100:  0.0023270335514098406\n",
      "Epoch 1150:  0.002327023772522807\n",
      "Epoch 1200:  0.0023270153906196356\n",
      "Epoch 1250:  0.002327009104192257\n",
      "Epoch 1300:  0.0023270014207810163\n",
      "Epoch 1350:  0.002326994203031063\n",
      "Epoch 1400:  0.002326984191313386\n",
      "Epoch 1450:  0.00232697487808764\n",
      "Epoch 1500:  0.0023269657976925373\n",
      "Epoch 1550:  0.002326957881450653\n",
      "Epoch 1600:  0.002326950430870056\n",
      "Epoch 1650:  0.0023269441444426775\n",
      "Epoch 1700:  0.002326939022168517\n",
      "Epoch 1750:  0.002326932270079851\n",
      "Epoch 1800:  0.0023269248194992542\n",
      "Epoch 1850:  0.002326916204765439\n",
      "Epoch 1900:  0.002326909452676773\n",
      "Epoch 1950:  0.002326903399080038\n",
      "Epoch 1999:  0.0023268908262252808\n"
     ]
    }
   ],
   "source": [
    "### using lbfgs ###\n",
    "print(\"using lbfgs...\")\n",
    "epochs = 100\n",
    "don.train()\n",
    "don.physics_calculator.set_learnable_coefficients(False)\n",
    "lbfgs2 = torch.optim.LBFGS(don.parameters(), lr=0.1, \n",
    "                          max_iter=500, max_eval=500, history_size=300, \n",
    "                          line_search_fn='strong_wolfe')\n",
    "\n",
    "for i in range(epochs):\n",
    "    def closure2():\n",
    "        if torch.is_grad_enabled(): \n",
    "            lbfgs2.zero_grad()\n",
    "        l = don.weak_loss(X_train[:, 0:1], X_train[:, 1:2], u0, y_train)\n",
    "        if l.requires_grad: \n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    lbfgs2.step(closure2)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    if (i%50)==0 or i==epochs-1:\n",
    "        l = closure2()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "don.eval()\n",
    "pred = don(X_train[:, 0:1], X_train[:, 1:2], u0).detach().numpy()\n",
    "print(BIC_AIC(pred, y_train.detach().numpy(), com))\n",
    "print(don.physics_calculator.coefficients.detach().numpy())\n",
    "print(don.weak_coeff_buffer)\n",
    "\n",
    "### using non-lbfgs ###\n",
    "print(\"using non-lbfgs...\")\n",
    "epochs = 2000 # 1000 to 2000 Okay\n",
    "don.train()\n",
    "don.physics_calculator.set_learnable_coefficients(False)\n",
    "# optimizer = Adam(pinn.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "optimizer = Adan(don.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    l = don.weak_loss(X_train[:, 0:1], X_train[:, 1:2], u0, y_train)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    if (i%50)==0 or i==epochs-1:\n",
    "        l = don.weak_loss(X_train[:, 0:1], X_train[:, 1:2], u0, y_train)\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5db4c-235e-42ad-9e94-7f8ee1ad0a2c",
   "metadata": {},
   "source": [
    "#### learn (pinn.physics_calculator.set_learnable_coefficients(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3415537-5acf-4f6b-9e73-8166a1c73be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### using non-lbfgs ###\n",
    "# epochs = 1000\n",
    "# pinn.train()\n",
    "# pinn.physics_calculator.set_learnable_coefficients(True)\n",
    "# # optimizer = Adam(pinn.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "# optimizer = Adan(pinn.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     l = pinn.loss(X_train[:, 0:1], X_train[:, 1:2], y_train)\n",
    "#     l.backward()\n",
    "#     optimizer.step()\n",
    "#     if (i%50)==0 or i==epochs-1:\n",
    "#         l = pinn.loss(X_train[:, 0:1], X_train[:, 1:2], y_train)\n",
    "#         print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "### using lbfgs ###\n",
    "# epochs = 100\n",
    "# pinn.train()\n",
    "# pinn.physics_calculator.set_learnable_coefficients(True)\n",
    "# lbfgs2 = torch.optim.LBFGS(pinn.parameters(), lr=0.1, \n",
    "#                           max_iter=500, max_eval=500, history_size=300, \n",
    "#                           line_search_fn='strong_wolfe')\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     def closure2():\n",
    "#         if torch.is_grad_enabled(): \n",
    "#             lbfgs2.zero_grad()\n",
    "#         l = pinn.loss(X_train[:, 0:1], X_train[:, 1:2], y_train)\n",
    "#         if l.requires_grad: \n",
    "#             l.backward()\n",
    "#         return l\n",
    "\n",
    "#     lbfgs2.step(closure2)\n",
    "\n",
    "#     # calculate the loss again for monitoring\n",
    "#     if (i%50)==0 or i==epochs-1:\n",
    "#         l = closure2()\n",
    "#         print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b761000-0dfa-472b-a07c-6d75d51afba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582100c-c27f-4cc3-bf31-48de02e21c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b40b39-5aa5-44d1-88b9-cc47fa93c2c1",
   "metadata": {},
   "source": [
    "#### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdd38d55-1674-4c54-89b5-6e54361dc6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-21083.125363570798, -21096.693075364085)\n",
      "[ 0.09517879 -1.0106909 ]\n",
      "[ 0.09903233 -1.00017205]\n"
     ]
    }
   ],
   "source": [
    "don.eval()\n",
    "pred = don(X_train[:, 0:1], X_train[:, 1:2], u0).detach().numpy()\n",
    "print(BIC_AIC(pred, y_train.detach().numpy(), com))\n",
    "print(don.physics_calculator.coefficients.detach().numpy())\n",
    "print(don.weak_coeff_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de065d4f-ee25-49e5-b057-d0a5afa36573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(don.state_dict(), \"../PMS_weights/don_mlp_sine_weak.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8df2aaca-0473-42a0-9cc9-6a5b809d935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.9451500000000004, 1.8760600000000056)\n",
      "(0.49756700000000986, 0.48153300000000065)\n",
      "(0.5522545000000086, 0.4677355000000035)\n",
      "(0.5464420000000011, 0.45889800000000613)\n",
      "(0.49243750000000086, 0.47523250000000294)\n"
     ]
    }
   ],
   "source": [
    "def percent_coeff_error(pred):\n",
    "    ground = np.array([0.1, -1])\n",
    "    errs = 100*np.abs(np.array(pred)-ground)/np.abs(ground)\n",
    "    return errs.mean(), errs.std()\n",
    "print(percent_coeff_error([ 0.09517879, -1.0106909 ]))\n",
    "print(percent_coeff_error([ 0.0990209, -1.00016034]))\n",
    "print(percent_coeff_error([ 0.09898001, -0.99915481]))\n",
    "print(percent_coeff_error([ 0.09899466, -0.99912456]))\n",
    "print(percent_coeff_error([0.09903233, -1.00017205]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c22a8-6da9-4470-b9b7-f1580ea9ec59",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "    - 3 main files are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51b2d94-abd4-4291-8c2f-8fa63affdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(pinn.state_dict(), \"../PMS_weights/pinn_mlp_sine_weak.pth\")\n",
    "# (-21077.95679626977, -21091.524508063056)\n",
    "# [[ 0.09809884]\n",
    "#  [-1.00043748]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a5fb606-a4bb-4e54-915c-157d8f574cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Tanh(): (-20637.284432281755, -20650.81253882045)\n",
    "# Sine(): (-20681.104197432018, -20694.632303970713)\n",
    "# SirenNet(dim_in = 2, dim_hidden = 50, dim_out = 1, num_layers = 4, w0_initial = 30.): (-21585.25976244083, -21598.787868979525)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysr]",
   "language": "python",
   "name": "conda-env-pysr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
